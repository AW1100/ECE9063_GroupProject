








g_epochs = 10


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

import os
from PIL import Image
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras.regularizers import l2
from tensorflow.keras.layers import Dropout
from tensorflow.keras.metrics import Precision, Recall
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.utils.class_weight import compute_class_weight
from tensorflow.keras.preprocessing.image import load_img, img_to_array


from skopt import BayesSearchCV
from sklearn.model_selection import StratifiedKFold
import keras
from sklearn.base import BaseEstimator, ClassifierMixin
from skopt.plots import plot_convergence
from skopt.plots import plot_objective





folder_path = './Data/images/cropped'
target_size = (224, 224)
ignore_small = False
factor = 0.1


def load_data_from_folder(folder):
    images = []
    labels = []
    for filename in os.listdir(folder):
        img_path = os.path.join(folder, filename)
        img = load_img(img_path, target_size=None)
        if ignore_small and (img.width < target_size[0]*factor or img.height < target_size[1]*factor):
            continue
        img = img.resize(target_size)
        img_array = img_to_array(img)/255
        label = 1 if filename.endswith('T.png') else 0  # 1 for 'T' (with helmet), 0 for 'F' (without helmet)
        images.append(img_array)
        labels.append(int(label))
    return np.array(images), np.array(labels)


images, labels = load_data_from_folder(folder_path)


X_train, X_test, y_train, y_test = train_test_split(
    images, labels, test_size=0.2, random_state=42
)


class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(enumerate(class_weights))


print(class_weights)
print(class_weight_dict)


X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)


print(X_train.shape)
print(X_test.shape)


unique_elements, counts = np.unique(y_train, return_counts=True)

# Print the results
for element, count in zip(unique_elements, counts):
    print(f"{element}: {count} occurrences")


def plot_cost_function(history):
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    plt.ylim(0.6, 1.0)
    # Add labels and title
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('CNN Training Accuracy')
    plt.legend()

    # Show the plot
    plt.show()


print(y_val.shape)
print(y_test.shape)


def print_metrics(model):
    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {test_accuracy * 100:.2f}%")

    y_pred = np.round(model.predict(X_test)).astype(int)
    cm = confusion_matrix(y_test,y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Without Helmet', 'With Helmet'])
    disp.plot(cmap='Blues')
    plt.title('Confusion Matrix')
    plt.show()

    f1 = f1_score(y_test, y_pred)
    print(f"F1 Score: {f1:.4f}")


additional_metrics = [Precision(), Recall()]


early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)











model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)


model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))


model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs,class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)





model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)


model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)





model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)


model = Sequential()
model.add(Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(0.01)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.002), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=g_epochs, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)








class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, num_filters_0=16, num_filters_1=16, num_filters_2=16, num_units_0=16, num_units_1=16):
        self.num_filters_0 = num_filters_0
        self.num_filters_1 = num_filters_1
        self.num_filters_2 = num_filters_2
        self.num_units_0 = num_units_0
        self.num_units_1 = num_units_1
        self.classes_ = [0, 1]
        self.model = self.create_model()

    def create_model(self):
        model = Sequential()
        model.add(Conv2D(self.num_filters_0, kernel_size=(3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D(self.num_filters_1, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Conv2D(self.num_filters_2, kernel_size=(3, 3), activation='relu'))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Flatten())
        model.add(Dense(self.num_units_0, activation='relu'))
        model.add(Dense(self.num_units_1, activation='relu'))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
        return model

    def fit(self, X, y):
        global X_val, y_val
        #X_val, y_val = X_val, y_val
        print(self.num_filters_0, self.num_filters_1, self.num_filters_2, self.num_units_0, self.num_units_1)
        self.model.fit(X, y, epochs=10,class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None, verbose=0)
        return self

    def predict(self, X):
        return np.round(self.model.predict(X))


hp_space = {
    'num_filters_0': (16, 256),
    'num_filters_1': (16, 256),
    'num_filters_2': (16, 256),
    'num_units_0' : (16, 256),
    'num_units_1' : (16, 256),
}


def val_accuracy_scorer(estimator, X, y):
    y_pred = estimator.predict(X)
    val_accuracy = accuracy_score(y, y_pred)
    print("Score: ",val_accuracy)
    return val_accuracy


keras_model_wrapper = KerasClassifierWrapper()


opt = BayesSearchCV(
    keras_model_wrapper,
    hp_space,
    n_iter=50,
    cv=3,
    scoring=val_accuracy_scorer,
    random_state=42,
)


opt.fit(X_train, y_train)


plt.figure()
plot_objective(opt.optimizer_results_[0])
plt.xlabel('Iteration')
plt.ylabel('Objective Function Value')
plt.show()


best_params = opt.best_params_
print(best_params)


def opt_cnn_model(num_filters_0, num_filters_1, num_filters_2, num_units_0, num_units_1):
    model = Sequential()
    model.add(Conv2D(num_filters_0, kernel_size=(3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(num_filters_1, kernel_size=(3, 3), activation='relu',kernel_regularizer=l2(0.01)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(num_filters_2, kernel_size=(3, 3), activation='relu',kernel_regularizer=l2(0.01)))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(num_units_0, activation='relu'))
    model.add(Dense(num_units_1, activation='relu'))
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'] + additional_metrics)
    return model


model = opt_cnn_model(114, 191, 240, 92, 177)
history = model.fit(X_train, y_train, epochs=10,class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)
print_metrics(model)
plot_cost_function(history)


class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, learning_rate=0.001, num_conv_layers=1, num_filters_0=16, num_filters_1=16, num_dense_layers=1, num_units_0=16, l2_reg=0.0, dropout=0.0):
        self.learning_rate = learning_rate
        self.num_conv_layers = num_conv_layers
        self.num_filters_0 = num_filters_0
        self.num_filters_1 = num_filters_1
        self.num_dense_layers = num_dense_layers
        self.num_units_0 = num_units_0
        self.l2_reg = l2_reg
        self.dropout = dropout
        self.classes_ = [0, 1]
        self.model = self.create_model()

    def create_model(self):
        model = Sequential()
        model.add(Conv2D(self.num_filters_0, kernel_size=(3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        for _ in range(self.num_conv_layers):
            model.add(Conv2D(self.num_filters_1, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(self.l2_reg)))
            model.add(MaxPooling2D(pool_size=(2, 2)))
            model.add(Dropout(self.dropout))
        model.add(Flatten())
        for _ in range(self.num_dense_layers):
            model.add(Dense(self.num_units_0, activation='relu'))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=self.learning_rate), metrics=['accuracy'])
        return model

    def fit(self, X, y):
        global X_val, y_val
        #X_val, y_val = X_val, y_val
        print(self.learning_rate, self.num_conv_layers, self.num_filters_0, self.num_filters_1, self.num_dense_layers, self.num_units_0, self.l2_reg, self.dropout)
        self.model.fit(X, y, epochs=10,class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None, verbose=0)
        return self

    def predict(self, X):
        return np.round(self.model.predict(X))


hp_space = {
    'learning_rate': (0.00001, 0.1),
    'num_conv_layers': (1, 5),
    'num_filters_0': (16, 256),
    'num_filters_1': (16, 256),
    'num_dense_layers': (1,3),
    'num_units_0': (16, 256),
    'l2_reg': (0.0, 0.3),
    'dropout': (0.0, 0.5)
}


keras_model_wrapper = KerasClassifierWrapper()


opt = BayesSearchCV(
    keras_model_wrapper,
    hp_space,
    n_iter=150,
    cv=3,
    scoring=val_accuracy_scorer,
    random_state=42,
)
opt.fit(X_train, y_train)


plt.figure()
plot_objective(opt.optimizer_results_[0])
plt.xlabel('Iteration')
plt.ylabel('Objective Function Value')
plt.show()


best_params = opt.best_params_
print(best_params)


class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, num_conv_layers=1, num_filters_0=16, num_filters_1=16, num_dense_layers=1, num_units_0=16, l2_reg=0.0):
        self.num_conv_layers = num_conv_layers
        self.num_filters_0 = num_filters_0
        self.num_filters_1 = num_filters_1
        self.num_dense_layers = num_dense_layers
        self.num_units_0 = num_units_0
        self.l2_reg = l2_reg
        self.classes_ = [0, 1]
        self.model = self.create_model()

    def create_model(self):
        model = Sequential()
        model.add(Conv2D(self.num_filters_0, kernel_size=(3, 3), activation='relu', input_shape=(target_size[0], target_size[1], 3)))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        for _ in range(self.num_conv_layers):
            model.add(Conv2D(self.num_filters_1, kernel_size=(3, 3), activation='relu', kernel_regularizer=l2(self.l2_reg)))
            model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(Flatten())
        for _ in range(self.num_dense_layers):
            model.add(Dense(self.num_units_0, activation='relu'))
        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])
        return model

    def fit(self, X, y):
        global X_val, y_val
        #X_val, y_val = X_val, y_val
        print(self.num_conv_layers, self.num_filters_0, self.num_filters_1, self.num_dense_layers, self.num_units_0, self.l2_reg)
        self.model.fit(X, y, epochs=10,class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None, verbose=0)
        return self

    def predict(self, X):
        return np.round(self.model.predict(X))

hp_space = {
    'num_conv_layers': (1, 5),
    'num_filters_0': (16, 256),
    'num_filters_1': (16, 256),
    'num_dense_layers': (1,3),
    'num_units_0': (16, 256),
    'l2_reg': (0.0, 0.3),
}


keras_model_wrapper = KerasClassifierWrapper()
opt = BayesSearchCV(
    keras_model_wrapper,
    hp_space,
    n_iter=150,
    cv=3,
    scoring=val_accuracy_scorer,
    random_state=42,
)
opt.fit(X_train, y_train)


best_params = opt.best_params_
print(best_params)


plt.figure()
plot_objective(opt.optimizer_results_[0])
plt.xlabel('Iteration')
plt.ylabel('Objective Function Value')
plt.show()


model = Sequential()
model.add(Conv2D(144, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(20, (3, 3), activation='relu', kernel_regularizer=l2(0.19548983724418573)))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(16, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=10, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)


model = Sequential()
model.add(Conv2D(29, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(153, (3, 3), activation='relu', kernel_regularizer=l2(0.27610129222292923)))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.34009735662630886))
model.add(Conv2D(153, (3, 3), activation='relu', kernel_regularizer=l2(0.27610129222292923)))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.34009735662630886))
model.add(Conv2D(153, (3, 3), activation='relu', kernel_regularizer=l2(0.27610129222292923)))
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.34009735662630886))
model.add(Flatten())
model.add(Dense(31, activation='relu'))
model.add(Dense(31, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer=Adam(learning_rate=0.07332129512512003), loss='binary_crossentropy', metrics=['accuracy'] + additional_metrics)

history = model.fit(X_train, y_train, epochs=10, class_weight=class_weight_dict, validation_data=(X_val, y_val), callbacks=None)

print_metrics(model)
plot_cost_function(history)








class KerasClassifierWrapper_mlp(BaseEstimator, ClassifierMixin):
    def __init__(self, num_units_0=128, num_units_1=256, num_units_2=256):
        self.num_units_0 = num_units_0
        self.num_units_1 = num_units_1
        self.num_units_2 = num_units_2
        self.classes_ = [0, 1]
        self.model = self.create_model()

    def create_model(self):
        model = Sequential()
        model.add(Flatten(input_shape=(target_size[0], target_size[1], 3)))  # Flatten the images
        model.add(Dense(self.num_units_0, activation='relu'))  
        model.add(Dense(self.num_units_1, activation='relu'))
        model.add(Dense(self.num_units_2, activation='relu'))   
        model.add(Dense(1, activation='sigmoid'))

        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0005), metrics=['accuracy'])
        return model

    def fit(self, X, y):
        global X_val, y_val
        #X_val, y_val = X_val, y_val
        print(self.num_units_0,self.num_units_1,self.num_units_2)
        self.model.fit(X, y, epochs=10, validation_data=(X_val, y_val), callbacks=None)
        return self

    def predict(self, X):
        return np.round(self.model.predict(X))


def val_accuracy_scorer_mlp(estimator, X, y):
    y_pred = estimator.predict(X)
    val_accuracy = accuracy_score(y, y_pred)
    print("Score: ",val_accuracy)
    return val_accuracy


hp_space_mlp= {
    'num_units_0' : (16, 128),
    'num_units_1' : (16, 128),
    'num_units_2' : (16, 128),
}


keras_model_wrapper = KerasClassifierWrapper_mlp()


opt = BayesSearchCV(
    keras_model_wrapper,
    hp_space_mlp,
    n_iter=10,
    cv=3,
    scoring=val_accuracy_scorer_mlp,
    random_state=42,
)


opt.fit(X_train, y_train)


best_params = opt.best_params_
print(best_params)


def opt_mpl_model(learning_rate,num_units_0, num_units_1,num_units_2):
    model = Sequential()
    model.add(Flatten(input_shape=(target_size[0], target_size[1], 3)))  # Flatten the images
        # Add Dense layers
    model.add(Dense(num_units_0, activation='relu'))  
    model.add(Dense(num_units_1, activation='relu'))
    model.add(Dense(num_units_2, activation='relu'))   
    model.add(Dense(1, activation='sigmoid'))

    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'] + additional_metrics)
    return model


def plot_cost_function_mlp(history):
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()
    plt.figure(figsize=(8, 5))
    plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
    # Add labels and title
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('MLP Training Accuracy')
    plt.legend()

    # Show the plot
    plt.show()


model = opt_mpl_model(0.001,62,98,120)
history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), callbacks=None)
print_metrics(model)
plot_cost_function(history)



